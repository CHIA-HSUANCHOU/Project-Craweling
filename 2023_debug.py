# -*- coding: utf-8 -*-
import requests
import time
import json
import random
import sys
import re
from collections import defaultdict, Counter
from bs4 import BeautifulSoup
from collections import OrderedDict
from datetime import datetime

PTT_URL = 'https://www.ptt.cc'
BOARD = 'Beauty'
session = requests.Session()
session.cookies.set('over18', '1')

### 1. Crawl
articles = []
popular_articles = []

def is_valid_title(title):
    if not title:
        return False
    title = title.strip()
    if title == "":
        return False
    if '[å…¬å‘Š]' in title or 'Fw:[å…¬å‘Š]' in title:
        return False
    return True

def parse_index_page(page_url, is_first_page=False, is_last_page=False):
    res = session.get(page_url, timeout=5)
    if res.status_code != 200:
        print(f"ç„¡æ³•å–å¾—é é¢: {page_url}")
        return None
    soup = BeautifulSoup(res.text, 'html.parser')
    entries = soup.select('div.r-ent')

    for entry in entries:
        title_tag = entry.select_one('div.title > a')

        if not title_tag:
            continue
        title = title_tag.text.strip()

        if not is_valid_title(title):
            continue
        url = title_tag['href']
        
        push_tag = entry.select_one('div.nrec').text.strip()
        full_url = PTT_URL + url

        date_tag = entry.select_one('div.date')
        if not date_tag:
            continue

        raw_date = date_tag.text.strip()

        try:
            month, day = map(int, raw_date.split('/'))
            #print(f"month{month}, day{day}")
            if is_first_page and month != 1:
                continue
                # âœ… æœ€å¾Œä¸€é ï¼Œé‡åˆ° 1/1 â†’ ä»£è¡¨ 12/31 çµæŸäº†ï¼Œæ•´é ä¸å†ç¹¼çºŒæŠ“
            if is_last_page and month == 1 and day == 1:
                print("âš ï¸ é‡åˆ° 1/1ï¼Œåœæ­¢è®€å–æ­¤é ")
                break

            date_str = f"{month:02d}{day:02d}"  # è½‰æˆ "MMDD"
        except:
            continue

        post = {
            'date': date_str,
            'title': title,
            'url': full_url
        }

        articles.append(post)

        if push_tag in ['çˆ†'] or (push_tag.isdigit() and int(push_tag) >= 100):
            popular_articles.append(post)

    return soup


def detect_date_on_page(index):
    url = f"{PTT_URL}/bbs/{BOARD}/index{index}.html"
    res = session.get(url, timeout=5)
    if res.status_code != 200:
        print(f"âŒ ç„¡æ³•è®€å– index{index}")
        return []

    soup = BeautifulSoup(res.text, 'html.parser')
    entries = soup.select('div.r-ent')

    dates = []
    for entry in entries:
        date_tag = entry.select_one('div.date')
        if not date_tag:
            continue
        try:
            raw_date = date_tag.text.strip()
            month, day = map(int, raw_date.split('/'))
            dates.append(datetime(2023, month, day))
        except:
            continue
    return dates


def find_start_index():
    idx = 3371  #3371
    last_valid = None
    while idx > 3000:
        dates = detect_date_on_page(idx)
        if dates is None:
            print(f"âš ï¸ index{idx} ç„¡æ³•å–å¾—è³‡æ–™ï¼Œè·³é")
            idx -= 1
            continue

        print(f"ğŸ” æª¢æŸ¥ index{idx}ï¼Œæ‰¾åˆ°æ—¥æœŸå€‘ï¼š{dates}")
        if any(d == datetime(2023, 1, 1) for d in dates):
            last_valid = idx  # æ‰¾åˆ°å« 1/1 çš„é é¢
        elif last_valid is not None:
            # å·²ç¶“éäº†å« 1/1 çš„æœ€å¾Œä¸€é  â†’ åœæ­¢å¾€å‰æ‰¾
            break

        idx -= 1
        time.sleep(random.uniform(0.3, 0.8))

    if last_valid is not None:
        print(f"âœ… æ‰¾åˆ°ç¬¬ä¸€é å« 2023/01/01 çš„ indexï¼š{last_valid}")
    else:
        print("âŒ æ²’æ‰¾åˆ°å« 2023/01/01 çš„é é¢")
    return last_valid



def find_end_index():
    idx = 3647  #3647
    while idx < 4000:
        dates = detect_date_on_page(idx)
        print(f"ğŸ” æª¢æŸ¥ index{idx}ï¼Œæ‰¾åˆ°æ—¥æœŸå€‘ï¼š{dates}")
        if any(d == datetime(2023, 12, 31) for d in dates):
            print(f"âœ… æ‰¾åˆ° 2023/12/31 åœ¨ index{idx}")
            return idx
        idx -= 1
        time.sleep(random.uniform(0.3, 0.8))
    return None

def main():
    start_time = time.time()
    start_idx = find_start_index() #2023
    end_idx = find_end_index()   #2023

    print(f"âœ… start_index = {start_idx}")
    print(f"âœ… end_index = {end_idx}")
    #start_idx = 3657 #2024
    #end_idx = 3926 #2024
    
    for i in range(start_idx, end_idx + 1):
        print(f"\nCrawling index {i} ...")
        page_url = f"{PTT_URL}/bbs/{BOARD}/index{i}.html"
        is_first = (i == start_idx)
        is_last = (i == end_idx)
        soup = parse_index_page(page_url, is_first_page=is_first, is_last_page=is_last)
        if soup is None:
            continue
        sleep_time = random.uniform(0.5, 1.5)
        print(f"ä¼‘æ¯ {sleep_time:.2f} ç§’é¿å…è¢«é– IP")
        time.sleep(sleep_time)

    with open("articles.jsonl", "w", encoding="utf-8") as f:
        for a in articles:
            json.dump(a, f, ensure_ascii=False)
            f.write("\n")

    with open("popular_articles.jsonl", "w", encoding="utf-8") as f:
        for a in popular_articles:
            json.dump(a, f, ensure_ascii=False)
            f.write("\n")

    end_time = time.time()
    duration = end_time - start_time
    print(f"\nâ± åŸ·è¡Œå®Œç•¢ï¼ç¸½è€—æ™‚ï¼š{duration:.2f} ç§’")
    print(f"\nçˆ¬èŸ²å®Œæˆï¼Œå…± {len(articles)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {len(popular_articles)} ç¯‡ç‚ºç†±é–€æ–‡ç« ã€‚")
    print("=== å‰ 5 ç¯‡æ–‡ç«  ===")
    for a in articles[:5]:
        print(a)
    print("=== å¾Œ 5 ç¯‡æ–‡ç«  ===")
    for a in articles[-5:]:
        print(a)

### 2. Push
def top10(counter):
    # å…ˆæŠŠ Counter è½‰æˆåˆ—è¡¨
    users = [{"user_id": u, "count": c} for u, c in counter.items()]
    # å…ˆç…§ user_id å­—å…¸åºå¤§åˆ°å°æ’ï¼ˆreverse æ’ user_idï¼‰
    users.sort(key=lambda x: x["user_id"], reverse=True)
    # å†ç…§ count å¤§åˆ°å°æ’ï¼ˆç©©å®šæ’åºï¼Œä¿ç•™å‰é¢å­—å…¸åºæ’åºçµæœï¼‰
    users.sort(key=lambda x: x["count"], reverse=True)
    return users[:10]

def run_push_analysis(start_date, end_date):
    start_time = time.time()  # è¨ˆæ™‚é–‹å§‹
    start_date = int(start_date)
    end_date = int(end_date)

    with open("articles.jsonl", "r", encoding="utf-8") as f:
        articles_data = [json.loads(line) for line in f]

    filtered_articles = [
        a for a in articles_data
        if "date" in a and start_date <= int(a["date"]) <= end_date
    ]

    print(f"âœ… åœ¨ {start_date} åˆ° {end_date} ä¹‹é–“ï¼Œå…±æ‰¾åˆ° {len(filtered_articles)} ç¯‡æ–‡ç« ")

    push_counter = Counter()
    boo_counter = Counter()
    push_total = 0
    boo_total = 0

    for idx, a in enumerate(filtered_articles):
        try:
            res = session.get(a["url"], timeout=5)
            if res.status_code != 200:
                continue
            soup = BeautifulSoup(res.text, 'html.parser')
            push_tags = soup.select("div.push")

            for tag in push_tags:
                tag_type = tag.select_one("span.push-tag")  # ä¾‹å¦‚ "æ¨", "å™“", "â†’"
                user_span = tag.select_one("span.push-userid") # ä½¿ç”¨è€… ID
                if not tag_type or not user_span:
                    continue

                tag_text = tag_type.text.strip()
                user = user_span.text.strip()

                if tag_text == "æ¨":
                    push_counter[user] += 1
                    push_total += 1
                elif tag_text == "å™“":
                    boo_counter[user] += 1
                    boo_total += 1

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è™•ç†æ–‡ç« ï¼š{a['url']}ï¼ŒéŒ¯èª¤ï¼š{e}")
            continue

        if (idx + 1) % 10 == 0:
            sleep_time = random.uniform(1, 1.5)
            print(f"ğŸ›Œ ç¬¬ {idx+1} ç¯‡ï¼Œä¼‘æ¯ {sleep_time:.2f} ç§’")
            time.sleep(sleep_time)


    result = {
        "push": {
            "total": push_total,
            "top10": top10(push_counter)
        },
        "boo": {
            "total": boo_total,
            "top10": top10(boo_counter)
        }
    }

    output_filename = f"push_{start_date:04d}_{end_date:04d}.json"
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ å·²è¼¸å‡ºçµæœè‡³ {output_filename}")

    end_time = time.time()
    duration = end_time - start_time
    print(f"\nğŸ“ å·²è¼¸å‡ºçµæœè‡³ {output_filename}")
    print(f"â±ï¸ åˆ†æç¸½è€—æ™‚ï¼š{duration:.2f} ç§’")


### 3. Popular 
def run_popular_analysis(start_date, end_date):
    start_time = time.time()
    start_date = int(start_date)
    end_date = int(end_date)

    with open("popular_articles.jsonl", "r", encoding="utf-8") as f:
        articles_data = [json.loads(line) for line in f]

    filtered_articles = [
        a for a in articles_data
        if "date" in a and start_date <= int(a["date"]) <= end_date
    ]

    print(f"âœ… åœ¨ {start_date} åˆ° {end_date} ä¹‹é–“ï¼Œå…±æ‰¾åˆ° {len(filtered_articles)} ç¯‡æ¨çˆ†æ–‡ç« ")

    image_urls = []
    img_pattern = re.compile(r'https?://[^\s]+\.(?:jpg|jpeg|png|gif)', re.IGNORECASE)

    for idx, a in enumerate(filtered_articles):
        try:
            res = session.get(a["url"], timeout=5)
            if res.status_code != 200:
                continue
            soup = BeautifulSoup(res.text, 'html.parser')

            # æ‰¾å…§æ–‡æ–‡å­—
            main_content = soup.select_one("#main-content")
            if main_content:
                text = main_content.get_text()
                image_urls += img_pattern.findall(text)

            # æ‰¾ç•™è¨€æ–‡å­—
            push_tags = soup.select("div.push span.push-content")
            for tag in push_tags:
                image_urls += img_pattern.findall(tag.text)

            if (idx + 1) % 5 == 0:
                sleep_time = random.uniform(0.5, 1.5)
                print(f"ğŸ›Œ å·²è™•ç†ç¬¬ {idx+1} ç¯‡ï¼Œä¼‘æ¯ {sleep_time:.2f} ç§’")
                time.sleep(sleep_time)

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è™•ç†æ–‡ç« ï¼š{a['url']}ï¼ŒéŒ¯èª¤ï¼š{e}")
            continue
    
    ###############################################
    #image_urls = list(OrderedDict.fromkeys(image_urls))

    result = {
        "number_of_popular_articles": len(filtered_articles),
        "image_urls": image_urls
    }

    output_filename = f"popular_{start_date:04d}_{end_date:04d}.json"
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ å·²è¼¸å‡ºçµæœè‡³ {output_filename}")
    print(f"â±ï¸ åˆ†æç¸½è€—æ™‚ï¼š{time.time() - start_time:.2f} ç§’")


### 4. Keyword
def extract_valid_text(text):
    lines = text.split("\n")
    start_idx = None
    end_idx = None
    for i, line in enumerate(lines):
        if start_idx is None and line.startswith("ä½œè€…"):
            start_idx = i
        if line.startswith("â€» ç™¼ä¿¡ç«™"):
            end_idx = i
            break
    if start_idx is not None and end_idx is not None:
        return "\n".join(lines[start_idx:end_idx])
    else:
        return None

def run_keyword_analysis(start_date, end_date, keyword):
    start_time = time.time()
    start_date = int(start_date)
    end_date = int(end_date)

    with open("articles.jsonl", "r", encoding="utf-8") as f:
        articles_data = [json.loads(line) for line in f]

    filtered_articles = [
        a for a in articles_data
        if "date" in a and start_date <= int(a["date"]) <= end_date
    ]

    print(f"âœ… åœ¨ {start_date} åˆ° {end_date} ä¹‹é–“ï¼Œå…±æ‰¾åˆ° {len(filtered_articles)} ç¯‡æ–‡ç« ")

    image_urls = []
    img_pattern = re.compile(r'https?://[^\s]+\.(?:jpg|jpeg|png|gif)', re.IGNORECASE)

    for idx, a in enumerate(filtered_articles):
        try:
            res = session.get(a["url"], timeout=5)
            if res.status_code != 200:
                continue
            soup = BeautifulSoup(res.text, 'html.parser')
            main_content = soup.select_one("#main-content")
            if main_content:
                text = main_content.get_text()
                valid_text = extract_valid_text(text)

                if valid_text and keyword in valid_text:
                    image_urls += img_pattern.findall(text)

                    push_tags = soup.select("div.push span.push-content")
                    for tag in push_tags:
                        image_urls += img_pattern.findall(tag.text)

            if (idx + 1) % 5 == 0:
                sleep_time = random.uniform(0.5, 1.5)
                print(f"ğŸ›Œ å·²è™•ç†ç¬¬ {idx+1} ç¯‡ï¼Œä¼‘æ¯ {sleep_time:.2f} ç§’")
                time.sleep(sleep_time)

        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è™•ç†æ–‡ç« ï¼š{a['url']}ï¼ŒéŒ¯èª¤ï¼š{e}")
            continue
    
    ###############################################
    #image_urls = list(OrderedDict.fromkeys(image_urls))

    result = {
        "image_urls": image_urls
    }

    output_filename = f"keyword_{start_date:04d}_{end_date:04d}_{keyword}.json"
    with open(output_filename, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ å·²è¼¸å‡ºçµæœè‡³ {output_filename}")
    print(f"â±ï¸ åˆ†æç¸½è€—æ™‚ï¼š{time.time() - start_time:.2f} ç§’")



if __name__ == "__main__":
    if len(sys.argv) >= 2:
        if sys.argv[1] == "crawl":
            main()
        elif sys.argv[1] == "push" and len(sys.argv) == 4:
            run_push_analysis(sys.argv[2], sys.argv[3])
        elif sys.argv[1] == "popular" and len(sys.argv) == 4:
            run_popular_analysis(sys.argv[2], sys.argv[3])
        elif sys.argv[1] == "keyword" and len(sys.argv) == 5:
            run_keyword_analysis(sys.argv[2], sys.argv[3],sys.argv[4])
        else:
            print("æŒ‡ä»¤éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ï¼š")
            print("  python 313657003.py crawl")
            print("  python 313657003.py push {start_date} {end_date}")
            print("  python 313657003.py popular {start_date} {end_date}")
            print("  python 313657003.py keyword {start_date} {end_date} {keyword}")

    else:
        print("è«‹è¼¸å…¥æ­£ç¢ºæŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼špython 313657003.py crawl æˆ– push")


